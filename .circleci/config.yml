version: 2.1
orbs:
  compass: atlassian-labs/compass@0.1.2
  maven: circleci/maven@1.3.0
  cypress: cypress-io/cypress@2.2.0
          
executors:
  with-chrome:
    docker:
      - image: 'cypress/browsers:node14.16.0-chrome90-ff88'
  base:
    docker:
      - image: cimg/deploy:2022.07
  jdk17:
    docker:
      - image: cimg/openjdk:17.0.3
    
    resource_class: xlarge
  python38:
    docker:
      - image: cimg/python:3.8


jobs:
  python-checkstyle:
    executor: python38
    steps:
      - checkout
      - run: pip install pylint
      - run:
          name: Lint Python
          command: pylint --rcfile=./.pylintrc ./src/*/*.py

  python-test:
    executor: python38
    steps:
      - checkout
      - run: mkdir test-reports
      - run:
          name: Test Python Services
          command: |
            for SERVICE in "contacts" "userservice"; do
              echo "testing $SERVICE..."
              # save current working dir to memory and cd to src/$SERVICE
              pushd src/$SERVICE
                python3 -m venv $HOME/venv-$SERVICE
                source $HOME/venv-$SERVICE/bin/activate
                pip install --upgrade pip
                pip install -r requirements.txt
                python -m pytest --junit-xml=../../test-reports/report-${SERVICE}.xml -v -p no:warnings
                deactivate
              # return to previously saved path
              popd
            done
      - store_test_results:
          path: test-reports
      - store_artifacts:
          path: test-reports

  java-checkstyle:
    executor: jdk17
    steps:
      - checkout
      - maven/with_cache:  
          verify_dependencies: false       
          steps: 
          - run: ./mvnw checkstyle:check


  java-test-and-code-cov:
    executor: jdk17
    steps:
      - checkout
      - run: mkdir test-reports
      - run: mvn help:effective-pom > /tmp/zpom.xml
      - restore_cache:
          keys:
            - v1-mvn-full-{{ checksum "/tmp/zpom.xml" }}
            - v1-mvn-full-
      - run: mvn dependency:go-offline
      - save_cache:
          key: v1-mvn-full-{{ checksum "/tmp/zpom.xml" }}
          paths:
            - ~/.m2
      - run: |
          ./mvnw test
          for SERVICE in "balancereader" "ledgerwriter" "transactionhistory"; do
          echo "checking $SERVICE..."
          # save current working dir to memory and cd to src/$SERVICE
          pushd src/$SERVICE
          ../../mvnw jacoco:report
          echo "Coverage for $SERVICE:"
          awk -F, \
          '{ instructions += $4 + $5; covered += $5 } END \
          { print covered, "/", instructions, " instructions covered"; \
          print int(100*covered/instructions), "% covered" }' \
          target/site/jacoco/jacoco.csv
          cp target/surefire-reports/*.xml ../../test-reports
          # return to previously saved path
          popd
          done
      - store_test_results:
          path: test-reports
      - store_artifacts:
          path: test-reports

  skaffold-build-push:
      executor: jdk17
      steps:
        - checkout
        - run: |
            curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
            unzip awscliv2.zip
            sudo ./aws/install
            curl -Lo skaffold https://storage.googleapis.com/skaffold/releases/latest/skaffold-linux-amd64 && \
            sudo install skaffold /usr/local/bin/
        - enable-oidc
        - setup_remote_docker:
            docker_layer_caching: true
        - run: mvn help:effective-pom > /tmp/zpom.xml
        - restore_cache:
            keys:
              - v1-mvn-full-{{ checksum "/tmp/zpom.xml" }}
              - v1-mvn-full-
        - run: |
            #this is temp use of admin rights while we bukld out nexus
            aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 483285841698.dkr.ecr.us-west-2.amazonaws.com
            # seems like skaffold jib needs a eird kick, hit me locally, passes on second run
            ./mvnw jib:_skaffold-fail-if-jib-out-of-date -Djib.requiredVersion=1.4.0 --projects src/ledgerwriter --also-make jib:_skaffold-files-v2 --quiet --batch-mode
            mkdir output
            skaffold build --build-concurrency=4 --tag=build<<pipeline.number>> --default-repo=483285841698.dkr.ecr.us-west-2.amazonaws.com --file-output=output/tags.json
            ls /tmp/
        - persist_to_workspace:
            root: output
            paths: [ 'tags.json' ]
 

  deploy:
    executor: base
    parameters:
      environment:
        type: string
        default: dev
        description: Environment suffix used by namesapce and SA account name.
    environment:
      K8_USER: boa-pipeline-<<parameters.environment>>
      K8_URL: "https://82BD0213010A79DBD7EAF886E3E553E0.gr7.us-west-2.eks.amazonaws.com"
      K8_CLUSTER: cera-solutions-eng
    steps:
      - checkout
      - run: echo "Now using namespace ${K8_NAMESPACE}"
      - use_service_account
      - run: |
          sudo apt update && sudo apt install kubectl -y
          kubectl config view
          kubectl get serviceaccounts -n ${K8_NAMESPACE}
      # temp whike using ecr
      - run: | 
          curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
          unzip awscliv2.zip
          sudo ./aws/install
      - enable-oidc
      - attach_workspace:
          at: output
      - run: 
          name: Deploy BoA
          command: |
            curl -Lo skaffold https://storage.googleapis.com/skaffold/releases/latest/skaffold-linux-amd64 && \
            sudo install skaffold /usr/local/bin/
            skaffold deploy --default-repo=483285841698.dkr.ecr.us-west-2.amazonaws.com --namespace=${K8_NAMESPACE} --build-artifacts=output/tags.json
      - run:
          name: Wait for deployment
          command: |
            kubectl wait deployment -n ${K8_NAMESPACE} frontend --for condition=Available=True --timeout=90s
      - run: 
          name: Print Frontend URL
          command: kubectl get service frontend -n ${K8_NAMESPACE} | awk '{print $4}'
  e2e:
    executor: with-chrome
    steps:
      - checkout
      - run: 
          name: Run Cypress Tests
          command: |
            cd ui-tests
            npx cypress run \
            --config baseUrl=https://dev.cera.circleci-labs.com \
            --browser firefox \
            --reporter junit \
            --reporter-options "mochaFile=results/my-test-output-[hash].xml"
      - store_test_results:
          path: ui-tests/results
    
   
workflows:
  main:
    jobs:
      - java-checkstyle
      - java-test-and-code-cov
      - python-checkstyle
      - python-test
      - skaffold-build-push:
          requires: [ python-test, java-test-and-code-cov ]
          context: reference-arch-aws-oidc
      - deploy:
          name: Deploy Dev
          requires: [ skaffold-build-push ]
          context: [ reference-arch-aws-oidc, cera-boa-dev, compass-integration-bank-of-aion ]
          post-steps:
            - compass/notify_deployment:
                token_name: COMPASS_CCI_TOKEN
                environment_type: development
      - cypress/install:
          requires: [ Deploy Dev ]
          cache-key: 'cache2-{{ arch }}-{{ .Branch }}-{{ checksum "package.json" }}'
      - cypress/run:
          requires: [ cypress/install ]
          executor: with-chrome
          config: baseUrl=https://dev.cera.circleci-labs.com
          browser: firefox
          attach-workspace: yes
          store_artifacts: true
          post-steps:
            - store_test_results:
                path: cypress/results
      - e2e:
          requires: [ Deploy Dev ]
      - deploy:
          name: Deploy Production
          requires: [ e2e ]
          context: [ reference-arch-aws-oidc, cera-boa-prod, compass-integration-bank-of-aion ]
          filters:
            branches:
              only: [ main ]
          post-steps:
            - compass/notify_deployment:
                token_name: COMPASS_CCI_TOKEN
                environment_type: production
                environment: CERA-Cluster-Namer-Prod


commands:
  use_service_account:
  # This command uses the assigned context to hydrate a kubeconfig from the given service account.
  # These values are automatically injected into the 2 app contexts by the `reference-architecture` project
    steps:
      - run:
          command: |
            echo ${K8_CERT} | base64 -d > ca.crt
            kubectl config set-cluster ${K8_CLUSTER} --server=${K8_URL} --certificate-authority=ca.crt
            export DECODED_TOKEN=$(echo ${K8_TOKEN} | base64 -d) #kubectl prints an encoded value, MUST decode it to work.
            kubectl config set-credentials ${K8_USER} --token=${DECODED_TOKEN}
            kubectl config set-context default --user=${K8_USER}  --cluster=${K8_CLUSTER} --namespace ${K8_NAMESPACE}
            kubectl config use-context default
          name: Use K8s token & cert to create kubeconfig.

  enable-oidc:
    steps:
      - run:
          name: authenticate-and-interact
          command: |
            # use the OpenID Connect token to obtain AWS credentials
            read -r AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SESSION_TOKEN \<<< \
              $(aws sts assume-role-with-web-identity \
              --role-arn ${AWS_ROLE_ARN} \
              --role-session-name "CircleCI-Pipeline-BoA" \
              --web-identity-token $CIRCLE_OIDC_TOKEN \
              --duration-seconds 3600 \
              --query 'Credentials.[AccessKeyId,SecretAccessKey,SessionToken]' \
              --output text)
            export AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SESSION_TOKEN
            # interact with AWS
            aws sts get-caller-identity
            echo "export AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}" >> $BASH_ENV
            echo "export AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}" >> $BASH_ENV
            echo "export AWS_SESSION_TOKEN=${AWS_SESSION_TOKEN}" >> $BASH_ENV
            source $BASH_ENV

# VS Code Extension Version: 1.0.0

# VS Code Extension Version: 1.0.0

# VS Code Extension Version: 1.1.1
